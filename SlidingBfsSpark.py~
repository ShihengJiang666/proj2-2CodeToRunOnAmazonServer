from pyspark import SparkContext
import Sliding, argparse
import math

def bfs_map(value):
    """
    value: Taken an element from RDD
    bfs_map function only applies children() to each element at the last level in RDD;
    return :If an element is not at the last level, then it will be put
    in an empty list and return;
    return :If an element is at the last level, then its children and 
    the element will be put into an empty list and return;
    """
    lst = []
    lst.append(value)
    value = (Sliding.hash_to_board(WIDTH,HEIGHT,value[0]), value[1])
    if (value[1] < level):
        return lst
    children = Sliding.children(WIDTH, HEIGHT, value[0])
    for each in children:
        lst.append(((Sliding.board_to_hash(WIDTH, HEIGHT, tuple(each))), value[1]+1))
    return lst

def revert_back(value):
    return str(value[1]) + " " + str(Sliding.hash_to_board(WIDTH, HEIGHT, value[0]))

def bfs_reduce(value1, value2):
    """
    value1 and value2: Taken two level numbers for elements which have the same key, 
    return: the smaller level number
    """
    return min(value1, value2)


def solve_puzzle(master, output, height, width, slaves):
    global HEIGHT, WIDTH, level
    HEIGHT=height
    WIDTH=width
    level = 0

    sc = SparkContext(master, "python")

    """ YOUR CODE HERE """
    sol = Sliding.board_to_hash(WIDTH, HEIGHT, Sliding.solution(WIDTH, HEIGHT))
    RDD = sc.parallelize([(sol, level)])
    counter = RDD.count()
    k, comp, data = 0, 0, 0
	repar = 0
    bound = (math.sqrt(WIDTH * HEIGHT)-1) * math.log(math.factorial(WIDTH * HEIGHT),2)
     
    # running mapreduce under lower bound
    while k <= bound:
        RDD = RDD.flatMap(bfs_map)
		if repar % 3 == 0:
			RDD = RDD.partitionBy(PARTITION_COUNT, hash)
        RDD = RDD.reduceByKey(bfs_reduce,PARTITION_COUNT)
        level += 1
        k += 1
		repar += 1
    k = 0
	repar = 0
    # running mapreduce until the number of elements in RDD stops increasing
    while True:
        RDD = RDD.flatMap(bfs_map)
		if repar % 3 == 0:
			RDD = RDD.partitionBy(PARTITION_COUNT, hash)
        RDD = RDD.reduceByKey(bfs_reduce,PARTITION_COUNT)
        if k % 3 == 0:
            comp = RDD.count()
            if comp == counter:
                break
            else: 
                counter = comp
        level += 1
        k += 1
		repar += 1
    # output code
    RDD = RDD.map(revert_back)
    RDD.coalesce(6).saveAsTextFile(output)
    sc.stop()



""" DO NOT EDIT PAST THIS LINE

You are welcome to read through the following code, but you
do not need to worry about understanding it.
"""

def main():
    """
    Parses command line arguments and runs the solver appropriately.
    If nothing is passed in, the default values are used.
    """
    parser = argparse.ArgumentParser(
            description="Returns back the entire solution graph.")
    parser.add_argument("-M", "--master", type=str, default="local[8]",
            help="url of the master for this job")
    parser.add_argument("-O", "--output", type=str, default="solution-out",
            help="name of the output file")
    parser.add_argument("-H", "--height", type=int, default=2,
            help="height of the puzzle")
    parser.add_argument("-W", "--width", type=int, default=2,
            help="width of the puzzle")
    parser.add_argument("-S", "--slaves", type=int, default=6,
            help="number of slaves executing the job")
    args = parser.parse_args()

    global PARTITION_COUNT
    PARTITION_COUNT = args.slaves * 16

    # call the puzzle solver
    solve_puzzle(args.master, args.output, args.height, args.width, args.slaves)

# begin execution if we are running this file directly
if __name__ == "__main__":
    main()
